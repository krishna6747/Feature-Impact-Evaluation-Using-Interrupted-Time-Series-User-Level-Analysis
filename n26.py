# -*- coding: utf-8 -*-
"""n26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NTYZf0ffw9Rhn-qcHZ_GaEHIoeEqdfeK
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("test_data_analyst_v2.csv")

df.head()

df.info()

# Drop useless index column
df = df.drop(columns=['Unnamed: 0'], errors='ignore')

# Convert dates, coercing bad values to NaT
df['signup_date'] = pd.to_datetime(df['signup_date'], format='mixed', errors='coerce')
df['month'] = pd.to_datetime(df['month'], format='mixed', errors='coerce')

# Check how many were coerced
print(df['signup_date'].isna().sum(), "bad signup_date values")
print(df['month'].isna().sum(), "bad month values")

df.head()

# Inspect the bad rows
df[df['signup_date'].isna()]

# Basic dataset overview
print("Shape:", df.shape)
print("Unique users:", df['user_id'].nunique())
print("Countries:", df['country_code'].unique())
print("Date range:", df['month'].min(), "to", df['month'].max())

# User present in months
df.groupby('user_id')['month'].nunique().describe()

# Null value check
df.isna().sum()

# Month on month null value distribution
df[df['feature_A'].isna()].groupby('month').size()

# nulls occurring in the same rows
df[df['feature_A'].isna()][
    ['feature_A','feature_B','feature_C','PAU','MAU']
].isna().mean()

print(f"Number of rows where 'month' is before 'signup_date': {(df['month'] < df['signup_date']).sum()}")

df[df['feature_A'].isna()].groupby(
    df['month'].dt.to_period('M')
).size()

# Mean distribution for null values against feature A
df['feature_A'].isna().groupby(df['month']).mean()

# User with null feature for feature A
null_users = df[df['feature_A'].isna()]['user_id'].nunique()
total_users = df['user_id'].nunique()

print("Users with null feature_A:", null_users)
print("Total users:", total_users)

# Create boolean mask once
is_null = df['feature_A'].isna()

# Count total months per user
user_total_months = df.groupby('user_id')['month'].count()

# Count null months per user (efficient way)
user_null_months = df.loc[is_null].groupby('user_id')['month'].count()

# Align indexes safely
null_ratio = (user_null_months / user_total_months).dropna()

print("Null months per affected user:")
print(user_null_months.describe())

print("\nNull ratio per affected user:")
print(null_ratio.describe())

"""After EDA & checking null values we can say

- Missing feature_A values are random logging gaps.

- They are small in magnitude.

- They do not cluster in time.

- They do not dominate any cohort.

- They affect only 1 month per user mostly.
"""

# Check if feature_A appears before July 2022
df[df['month'] < '2022-07-01']['feature_A'].sum()

# Adoption rate over time
adoption = df.groupby('month')['feature_A'].mean()

import matplotlib.pyplot as plt

plt.figure(figsize=(12,5))
plt.plot(adoption)
plt.axvline(pd.to_datetime("2022-07-01"), color='red', linestyle='--')
plt.title("Feature A Adoption Over Time")
plt.show()

# Checked for feature distribution beetween 1st aug 22 to 31st dec 22
df.groupby('month')['feature_A'].sum().loc['2022-08-01':'2022-12-31']

# MAU & PAU trend over time
monthly = df.groupby('month')[['MAU','PAU']].mean().reset_index()

import matplotlib.pyplot as plt

plt.figure(figsize=(12,5))
plt.plot(monthly['month'], monthly['MAU'], label='MAU')
plt.plot(monthly['month'], monthly['PAU'], label='PAU')
plt.axvline(pd.to_datetime("2022-07-01"), color='red', linestyle='--')
plt.legend()
plt.title("MAU & PAU Trend Over Time")
plt.show()

"""There Is a Clear Data Error
- There is a massive MAU drop to near 0 around that same period
- All valid months are end-of-month dates
but 2022-11-03 appears randomly.
"""

# made df not including the date 2022-11-03
df = df[df['month'] != '2022-11-03']

monthly = df.groupby('month')[['MAU','PAU']].mean().reset_index()

plt.figure(figsize=(12,5))
plt.plot(monthly['month'], monthly['MAU'], label='MAU')
plt.plot(monthly['month'], monthly['PAU'], label='PAU')
plt.axvline(pd.to_datetime("2022-07-01"), color='red', linestyle='--')
plt.legend()
plt.title("MAU & PAU Trend (Cleaned)")
plt.show()

# Interrupted Time Series regression - to measure whether an event caused a change in level or trend of a metric over time
import statsmodels.api as sm

monthly = df.groupby('month')['MAU'].mean().reset_index()

monthly['time'] = range(len(monthly))
monthly['post'] = (monthly['month'] >= '2022-07-01').astype(int)
monthly['time_post'] = monthly['time'] * monthly['post']

X = sm.add_constant(monthly[['time','post','time_post']])
y = monthly['MAU']

model = sm.OLS(y, X).fit()
print(model.summary())

import statsmodels.api as sm

X = sm.add_constant(monthly[['time','post','time_post']])
y = monthly['MAU']

model_hac = sm.OLS(y, X).fit(cov_type='HAC', cov_kwds={'maxlags':1})
print(model_hac.summary())

"""- Pre-trend

time = -0.0030
p < 0.001
MAU was declining ~0.29pp per month before launch.
This decline is highly significant and structural.

- Immediate Post-Launch Effect

post = -0.0401
p = 0.092. There is no statistically reliable immediate jump in MAU after launch

- Post-Launch Trend Change
time_post = 0.0008
p = 0.308 Not significant, No slope improvement after feature launch.

- No statistically significant evidence that Feature A improved overall MAU at the aggregate level.

- Adoption only ~5–6% → too small to shift macro metric
- Feature impacts depth, not breadth
- Feature mostly used by already-active users
- Macro Spain decline overpowering local uplift
"""

monthly_pau = df.groupby('month')['PAU'].mean().reset_index()

monthly_pau['time'] = range(len(monthly_pau))
monthly_pau['post'] = (monthly_pau['month'] >= '2022-07-01').astype(int)
monthly_pau['time_post'] = monthly_pau['time'] * monthly_pau['post']

import statsmodels.api as sm

X = sm.add_constant(monthly_pau[['time','post','time_post']])
y = monthly_pau['PAU']

model_pau = sm.OLS(y, X).fit(cov_type='HAC', cov_kwds={'maxlags':1})
print(model_pau.summary())

"""- Pre-Trend

time = +0.0003
p < 0.001  Before Feature A launch, PAU was already increasing gradually.
So unlike MAU (which was declining),
PAU had an improving structural trend.  

- Immediate Post-Launch Level Change

post = +0.0061
p = 0.018 Statistically significant at 5%. There was an immediate +0.61 percentage point increase in PAU after launch.  

- Post-Launch Trend Change

time_post = -0.0002
p = 0.001 Significant negative slope change.  


- After launch:

Immediate boost But growth rate slowed afterward

This suggests > A one-time engagement uplift,
not a sustained acceleration.

MAU    > No significant impact          
PAU    > Significant immediate increase
"""

first_use = (
    df[df['feature_A'] == 1]
    .groupby('user_id')['month']
    .min()
    .reset_index()
    .rename(columns={'month':'first_A_month'})
)

df = df.merge(first_use, on='user_id', how='left')

# Convert to year-month period
df['month_period'] = df['month'].dt.to_period('M')
df['first_A_period'] = df['first_A_month'].dt.to_period('M')

# Compute month difference
df['event_time'] = (
    df['month_period'] - df['first_A_period']
).apply(lambda x: x.n if pd.notnull(x) else np.nan)

event_df = df[
    (df['event_time'] >= -6) &
    (df['event_time'] <= 6)
]

event_pau = event_df.groupby('event_time')['PAU'].mean()
event_mau = event_df.groupby('event_time')['MAU'].mean()

event_summary = (
    event_df
    .groupby('event_time')[['PAU','MAU']]
    .mean()
    .reset_index()
)

import matplotlib.pyplot as plt

plt.figure(figsize=(12,5))
plt.plot(event_summary['event_time'], event_summary['PAU'], label='PAU')
plt.plot(event_summary['event_time'], event_summary['MAU'], label='MAU')
plt.axvline(0, color='red', linestyle='--')
plt.legend()
plt.title("Event Study Around First Feature A Usage")
plt.show()

"""First Observation — These Are Highly Active Users
- After looking at MAU line > Users who adopt Feature A are almost always active already.
- PAU looking stable At adoption > Slight increase That is a visible uplift

Their PAU probability increases after adoption.

Pre-trend is relatively flat.

Post-trend shows step-up.
"""

#Pre event analysis
pre_event = event_df[event_df['event_time'] < 0]

import statsmodels.api as sm

pre_summary = (
    pre_event.groupby('event_time')['PAU']
    .mean()
    .reset_index()
)

pre_summary['time'] = pre_summary['event_time']

X = sm.add_constant(pre_summary[['time']])
y = pre_summary['PAU']

pre_model = sm.OLS(y, X).fit()
print(pre_model.summary())

"""Pre-Trend Test (Before Adoption)
time = 0.0015, p = 0.198

Not statistically significant. There is no statistically significant upward trend in PAU before users adopt Feature A.

Macro (Interrupted Time Series)
- Metric	Result
- MAU	>  No impact
- PAU	> Significant immediate uplift
"""

# Monthly adoption rate
adoption = df.groupby('month')['feature_A'].mean().reset_index()

plt.figure(figsize=(12,5))
plt.plot(adoption['month'], adoption['feature_A'])
plt.axvline(pd.to_datetime("2022-07-01"), color='red', linestyle='--')
plt.title("Feature A Adoption Rate Over Time")
plt.ylabel("Adoption Rate")
plt.show()

print("Peak adoption:", adoption['feature_A'].max())
print("Latest adoption:", adoption['feature_A'].iloc[-1])

"""Adoption stabilized around 5–6%.

No exponential growth.

Limited macro penetration.
"""

# User ever adopted
ever_adopt = df.groupby('user_id')['feature_A'].max().mean()
print(f"Share of users who ever adopted Feature A: {ever_adopt * 100:.2f}%")

# Final - MAU Interrupted Time Series
import statsmodels.api as sm

monthly_mau = df.groupby('month')['MAU'].mean().reset_index()
monthly_mau['time'] = range(len(monthly_mau))
monthly_mau['post'] = (monthly_mau['month'] >= '2022-07-01').astype(int)
monthly_mau['time_post'] = monthly_mau['time'] * monthly_mau['post']

X = sm.add_constant(monthly_mau[['time','post','time_post']])
y = monthly_mau['MAU']

model_mau = sm.OLS(y, X).fit(cov_type='HAC', cov_kwds={'maxlags':1})
print(model_mau.summary())

print(f"Immediate MAU change: {model_mau.params['post'] * 100:.2f}%")
print(f"MAU slope change: {model_mau.params['time_post'] * 100:.2f}%")

# Counterfactual Line
monthly_mau['pred'] = model_mau.predict(X)

plt.figure(figsize=(12,5))
plt.plot(monthly_mau['month'], monthly_mau['MAU'], label='Actual')
plt.plot(monthly_mau['month'], monthly_mau['pred'], label='Fitted', linestyle='--')
plt.axvline(pd.to_datetime("2022-07-01"), color='red', linestyle='--')
plt.legend()
plt.title("MAU Interrupted Time Series Fit")
plt.show()

"""**PAU Impact**"""

# PAU Interrupted Time Series
monthly_pau = df.groupby('month')['PAU'].mean().reset_index()
monthly_pau['time'] = range(len(monthly_pau))
monthly_pau['post'] = (monthly_pau['month'] >= '2022-07-01').astype(int)
monthly_pau['time_post'] = monthly_pau['time'] * monthly_pau['post']

X = sm.add_constant(monthly_pau[['time','post','time_post']])
y = monthly_pau['PAU']

model_pau = sm.OLS(y, X).fit(cov_type='HAC', cov_kwds={'maxlags':1})
print(model_pau.summary())

baseline_pau = monthly_pau[monthly_pau['post']==0]['PAU'].mean()
uplift = model_pau.params['post']

print(f"Baseline PAU: {baseline_pau * 100:.2f}%")
print(f"Immediate uplift: {uplift * 100:.2f}%")
print(f"Relative uplift (%): {uplift / baseline_pau * 100:.2f}%")

"""**User-Level Impact**"""

# Build clean regression dataset explicitly
reg_df = df[['PAU_demeaned','feature_A_demeaned','user_id']].dropna().copy()

model_simple = smf.ols(
    'PAU_demeaned ~ feature_A_demeaned',
    data=reg_df
).fit(cov_type='cluster', cov_kwds={'groups': reg_df['user_id']})

print(model_simple.summary())

print(f"Average PAU: {df['PAU'].mean() * 100:.2f}%")

plt.figure(figsize=(12,5))
plt.plot(event_summary['event_time'], event_summary['PAU'])
plt.axvline(0, color='red', linestyle='--')
plt.title("PAU Around Feature A Adoption")
plt.show()

"""3-Month Rolling MAU"""

df = df.sort_values(['user_id','month'])

df['MAU_3m'] = (
    df.groupby('user_id')['MAU']
      .rolling(3, min_periods=1)
      .mean()
      .reset_index(level=0, drop=True)
)

df[['user_id','month','MAU','MAU_3m']].head(20)

"""**Adoption Cohort**"""

cohort = df[df['feature_A']==1].groupby('user_id')['month'].min()
cohort.head()

cohort_df = cohort.reset_index()
cohort_df.columns = ['user_id','first_A_month']
cohort_df.head()

# Retention curve
df = df.merge(cohort_df, on='user_id', how='left')
df['month_period'] = df['month'].dt.to_period('M')
df['first_A_period'] = df['first_A_month'].dt.to_period('M')

df['months_since_A'] = (
    df['month_period'] - df['first_A_period']
).apply(lambda x: x.n if pd.notnull(x) else None)

retention = (
    df[df['months_since_A'] >= 0]
    .groupby('months_since_A')['feature_A']
    .mean()
)

retention.head(10)

plt.figure(figsize=(10,5))
plt.plot(retention.index, retention.values)
plt.title("Feature A Retention Curve")
plt.xlabel("Months Since First Adoption")
plt.ylabel("Probability of Using Feature Again")
plt.show()

"""Feature A is high-quality and deepens engagement,
but limited adoption constrains company-level impact.

- Adoption

Stabilized at 5–6%

Limited macro penetration

- Retention

82% month-1 retention

~67% retention after 12+ months

Strong habit formation

- MAU

No statistically significant change

Pre-existing downward trend continued

- PAU

+0.6pp macro uplift (ITS)

+7.7pp within-user uplift

Significant and economically meaningful

**Why MAU Alone Is Not Sufficient**

MAU measures breadth of engagement (was the user active at least once in the month).

However, Feature A is a local payment feature, which is more likely to influence:

- Transaction frequency

- Money inflow/outflow behavior

- Financial stickiness

rather than simple app activity.

Therefore, MAU is a necessary but insufficient metric.

1. **PAU (Primary Account User)**
PAU measures how deeply someone is actually using their bank account things like regularly moving money in and out. It's a better way to tell if a customer is truly engaged, not just logged in.
When we looked at Feature A, it didn't change how many people were active (MAU), but it did meaningfully improve PAU. That tells us Feature A is making people use their account more seriously, which is what really matters for the business.
2. **Rolling Engagement (3-Month Average)**
Checking whether someone was active in just one month can be misleading people have off months. Looking at a 3-month rolling window gives a more honest picture of whether engagement actually improved or if it was just a one-time blip.
3. **Adoption & Retention**
Only about 5–6% of users tried Feature A, which is relatively low. But of those who did try it, over 65% were still using it a year later that's a strong sign people find it genuinely useful once they discover it.
In short: Feature A is good, it's just not reaching enough people yet.
4. **Within-User Behavior Change**
Rather than comparing different users to each other (which can be misleading), we look at whether the same person changed their behavior after using Feature A. This gives us a much cleaner read on whether the feature is actually driving change.

S**hould We Redefine PAU?**
The current PAU definition requires consistent money movement over at least 3 months. That's a solid long-term measure, but it's slow — it won't pick up early signs that a feature is working.
A better approach would be to track two things side by side: the existing 3-month PAU for long-term health, and a simpler monthly activity check for early signals. Together they give you both a quick pulse and a deeper read on engagement.

**Methods Used to Measure Impact**
Since Feature A wasn't tested with a proper experiment, we used three analytical approaches to estimate its effect.

**A. Before vs. After Analysis (ITS) - **
We compared user behavior before and after Feature A launched in July 2022. This works well because we had a clear launch date and a long history of data to compare against.
What we found: No change in general activity (MAU), but a meaningful lift in deep engagement (PAU). One caveat — we assumed nothing else major happened at the same time, and we had to remove one corrupted month of data.

**B. User Behavior Around First Use (Event Study)- **
We looked at how individual users behaved in the months before and after they first used Feature A.
What we found: Engagement was flat before adoption, then clearly improved after. This confirms the timing lines up with Feature A, not something else. The limitation is that users who chose to adopt may already be more engaged than average.

**C. Same-User Comparison (Fixed Effects) -**
The strongest method. Instead of comparing different users, we measured how the same user changed after using Feature A — controlling for the fact that some people are just naturally more active.
What we found: Using Feature A made someone about 4.4× more likely to become a PAU. That's a very strong result and highly statistically significant.
The main caveat is we can't rule out that people who use Feature A are also doing other things that drive engagement at the same time.

**What we Found**
Adoption — Only 5–6% of users have tried Feature A, which limits how much impact it can have at scale.
**General Activity (MAU)** — No meaningful change. The downward trend that existed before launch simply continued.
**Deep Engagement (PAU)** — Feature A gave a small but real boost at the overall level. More strikingly, users who actually use Feature A are 4.4× more likely to become deeply engaged compared to their own baseline.
**Retention **— 82% of users who try it come back the next month, and 67% are still using it after a year. That's a strong sign people genuinely find it useful.

**The Big Picture**
Feature A works well — it drives real engagement and sticks. The problem is not enough people are using it. The product is strong; the reach is weak.

**What We Recommend**
Push harder on getting more users to try Feature A. Given how well it retains and how much it lifts engagement, even a modest increase in adoption could have a meaningful impact on overall business health. The focus should shift from improving the feature to simply getting it in front of more people.
"""